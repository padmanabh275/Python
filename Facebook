# -*- coding: utf-8 -*-
"""
Created on Fri Aug 16 12:15:12 2019

@author: Training89
"""

#packages import
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from IPython import display

# Comment this if the data visualisations doesn't work on your side
%matplotlib inline

plt.style.use('bmh')
pd.options.display.max_columns=None

# Project Facebook Comments 

#packages import
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import metrics

#data load
df=pd.read_csv(r'C:\Users\Training89\Documents\Dataset.csv')
df.head()
df.info()

list(set(df.dtypes.tolist()))
df_num = df.select_dtypes(include = ['float64', 'int64'])
df_num.head()
df_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8); # ; avoid having the matplotlib verbose informations


df_num_corr = df_num.corr()['output'][:-1] # -1 because the latest row is SalePrice
golden_features_list = df_num_corr[abs(df_num_corr) > 0.5].sort_values(ascending=False)
print("There is {} strongly correlated values with Output:\n{}".format(len(golden_features_list), golden_features_list))


# Feature to Feature Correation 

corr = df_num.drop('output', axis=1).corr() # We already examined SalePrice correlations
plt.figure(figsize=(12, 10))

sns.heatmap(corr[(corr >= 0.5) | (corr <= -0.4)], 
            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,
            annot=True, annot_kws={"size": 8}, square=True);

                 
fig, axes = plt.subplots(round(len(df_num.columns) / 3), 3, figsize=(12, 30))

for i, ax in enumerate(fig.axes):
    if i < len(df_num.columns):
        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)
        sns.countplot(x=df_num.columns[i], alpha=0.7, data=df_num, ax=ax)

fig.tight_layout()
                 
                 
                 
                 
features_to_analyse = [x for x in quantitative_features_list if x in golden_features_list]
features_to_analyse.append('output')
features_to_analyse                 
fig, ax = plt.subplots(round(len(features_to_analyse) / 3), 3, figsize = (18, 12))

for i, ax in enumerate(fig.axes):
    if i < len(features_to_analyse) - 1:
        sns.regplot(x=features_to_analyse[i],y='output', data=df[features_to_analyse], ax=ax)                 
                 
                 
df['likes'].describe()
df['Category'].value_counts()
df['Checkins'].value_counts()
df['likes'].value_counts()
plt.figure(figsize=(12,10))
sns.heatmap(df.corr(),cmap='viridis',annot=True,cbar=False)
sns.distplot(df['likes'],bins=20)
plt.title("Distribution of Total Page Likes")

plt.figure(figsize=(14,8))
sns.distplot(df[df['likes']<1000]['likes'],bins=20,color='dodgerblue',hist_kws={'alpha':0.6})
plt.xlim(0,1000)
plt.xlabel("# of Likes",fontsize=15)
plt.ylabel('Frequency',fontsize=15)
plt.title('Distribution of Like/Post',fontsize=20)


sns.countplot(df['Category'],palette='viridis')


#Finding the Percentage missing data along with the values
total = df.isnull().sum().sort_values(ascending=False)
percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
print(missing_data)
#Null Replace - Removing Null with mode/ medain depedning on the data 
# Imputation of the data by replacing it with mean or mode etc
def null_impute(df):
    for cols in list(df.columns.values):
        if df[cols].isnull().sum() == 0:
            df[cols] = df[cols]
        elif df[cols].dtypes == 'float64' or df[cols].dtypes == 'int64':
            df[cols] = df[cols].fillna(df[cols].mean())
        else:
            df[cols] = df[cols].fillna(df[cols].mode()[0])
            
null_impute(df)
# Null removed in the dataset
df.isnull().sum()
df.isnull().sum().max()
